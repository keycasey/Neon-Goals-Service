#!/usr/bin/env python3
"""
AutoTrader scraper using Camoufox.

Accepts structured query format (from parse_vehicle_query.py) and
converts it to AutoTrader-specific parameters via the adapter function.

Features:
- Camoufox anti-detection browser
- Bot detection returns None for worker retry with VPN
"""
import json
import os
import sys
import tempfile
import logging
import re
import time
from pathlib import Path
from typing import Dict, Optional, List

logging.basicConfig(level=logging.ERROR, format='%(message)s', stream=sys.stderr)

# Try to import Playwright for CDP first
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# Try to import Camoufox as fallback
try:
    from camoufox.sync_api import Camoufox
    from browserforge.fingerprints import Screen
    CAMOUFOX_AVAILABLE = True
except ImportError:
    CAMOUFOX_AVAILABLE = False


def adapt_structured_to_autotrader(structured: dict) -> str:
    """
    Convert structured query format to AutoTrader URL.

    Handles two formats:
    1. LLM retailer-specific format: {url_params, path_components, body_params}
    2. Legacy format: {makes, models, trims, zip, location, year, ...}

    AutoTrader specifics:
    - URL format: /cars-for-sale/{make}/{model}/{trim}/{location}?{params}
    - Example: /cars-for-sale/gmc/sierra-3500/denali-ultimate/san-mateo-ca?endYear=2024&searchRadius=500

    Args:
        structured: The structured query dict

    Returns:
        AutoTrader URL string
    """
    base_url = "https://www.autotrader.com"

    # Handle LLM retailer-specific format (new format from parse_vehicle_query.py)
    if 'url_params' in structured or 'path_components' in structured:
        path_components = structured.get('path_components', {})
        url_params = structured.get('url_params', {})

        # Build path from path_components
        # IMPORTANT: Color comes FIRST in the path (before make)
        path_parts = ["cars-for-sale"]
        color = path_components.get('color', '')
        make = path_components.get('make', '')
        model = path_components.get('model', '')
        trim = path_components.get('trim', '')

        if color:
            path_parts.append(color.lower())
        if make:
            path_parts.append(make.lower())
        if model:
            path_parts.append(model.lower())
        if trim:
            path_parts.append(trim.lower())

        url_path = '/' + '/'.join(path_parts)

        # Build query params from url_params
        params = []
        for key, value in url_params.items():
            if value:  # Skip empty values
                params.append(f"{key}={value}")

        # CRITICAL: If searchRadius is specified, zip is REQUIRED
        has_search_radius = any('searchRadius' in p for p in params)
        has_zip = any('zip=' in p for p in params)
        if has_search_radius and not has_zip:
            # Add default zip if searchRadius but no zip provided
            params.append("zip=94002")  # Default to San Mateo, CA
        elif not has_search_radius and not has_zip:
            # If neither specified, add both with defaults
            params.append("searchRadius=500")
            params.append("zip=94002")

        query_string = '&'.join(params)
        return f"{base_url}{url_path}?{query_string}"

    # Handle legacy format {makes, models, trims, zip, location, year, ...}
    makes = structured.get('makes') or ([structured.get('make')] if structured.get('make') else [])
    make = makes[0].lower().replace(' ', '-') if makes else ''

    models = structured.get('models') or ([structured.get('model')] if structured.get('model') else [])
    model = models[0].lower().replace(' ', '-') if models else ''

    trims = structured.get('trims', [])
    trim = trims[0].lower().replace(' ', '-') if trims else ''

    # Get color (for path - comes before make)
    color = structured.get('color', '')
    if isinstance(color, list):
        color = color[0].lower() if color else ''
    elif color:
        color = color.lower()

    # Build base URL path
    # IMPORTANT: Color comes FIRST in the path (before make)
    path_parts = ["cars-for-sale"]
    if color:
        path_parts.append(color)
    if make:
        path_parts.append(make)
    if model:
        path_parts.append(model)
    if trim:
        path_parts.append(trim)

    # Add location to path - handle both location dict and zip code
    location = structured.get('location', {})
    zip_code = structured.get('zip')

    # If we have a zip code, skip adding location to path (use query param instead)
    if not zip_code:
        city = location.get('city', '').lower().replace(' ', '-') if location.get('city') else ''
        state = location.get('state', '').lower() if location.get('state') else ''

        if city or state:
            location_part = f"{city}-{state}" if city and state else (city or state)
            path_parts.append(location_part)

    url_path = '/' + '/'.join(path_parts)

    # Build query parameters
    params = []

    # Add zip code if provided
    if structured.get('zip'):
        params.append(f"zip={structured['zip']}")

    # Add search radius (use provided radius or default to 500)
    radius = structured.get('radius', 500)
    params.append(f"searchRadius={radius}")

    # Add year range
    year = structured.get('year', {})
    if isinstance(year, dict):
        if year.get('min'):
            params.append(f"startYear={year['min']}")
        if year.get('max'):
            params.append(f"endYear={year['max']}")
    elif isinstance(year, int):
        params.append(f"startYear={year}&endYear={year}")

    # Fallback to yearMin/yearMax if year dict not provided
    if structured.get('yearMin'):
        params.append(f"startYear={structured['yearMin']}")
    if structured.get('yearMax'):
        params.append(f"endYear={structured['yearMax']}")

    # Add max price if specified
    if structured.get('maxPrice'):
        params.append(f"maxPrice={structured['maxPrice']}")

    # Build final URL
    query_string = '&'.join(params)
    url = f"{base_url}{url_path}?{query_string}"

    return url


def extract_number(text: str) -> int:
    if not text:
        return 0
    cleaned = re.sub(r'[^\d,.]', '', str(text))
    cleaned = cleaned.replace(',', '')
    try:
        return int(float(cleaned))
    except:
        return 0


def scrape_with_playwright_cdp(query: str, max_results: int, cdp_url: str = "http://localhost:9222"):
    """Scrape using Playwright connected to Chrome CDP"""
    results = []

    with sync_playwright() as p:
        try:
            logging.error(f"[AutoTrader] Connecting to Chrome CDP at {cdp_url}...")
            browser = p.chromium.connect_over_cdp(cdp_url)

            # Use the first context or create a new one
            if browser.contexts:
                context = browser.contexts[0]
            else:
                context = browser.new_context()

            # Use existing page or create new one, close other pages
            if len(context.pages) > 0:
                # Close extra pages, keep only one
                while len(context.pages) > 1:
                    context.pages[-1].close()
                page = context.pages[0]
            else:
                page = context.new_page()

            logging.error(f"[AutoTrader] Connected via CDP!")
        except Exception as e:
            logging.error(f"[AutoTrader] CDP connection failed: {e}")
            return None

        try:
            logging.error(f"[AutoTrader] Query: {query}")

            # Use the query URL directly if it's a full URL
            if query.startswith('http'):
                search_url = query
            else:
                # Build search URL from text query
                search_terms_slug = query.replace(' ', '-').lower()
                search_url = f"https://www.autotrader.com/cars-for-sale/{search_terms_slug}/san-mateo-ca?searchRadius=500"

            logging.error(f"[AutoTrader] Navigating to: {search_url}")
            page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
            time.sleep(random.uniform(2, 4))

            # Wait for listings
            try:
                page.wait_for_selector('a[href*="/cars-for-sale/vehicle/"]', timeout=30000)
                logging.error(f"[AutoTrader] Vehicle links appeared")
            except:
                logging.error(f"[AutoTrader] Timeout waiting for vehicle links")

            # Infinite scroll
            last_listing_count = 0
            scroll_attempts = 0
            max_scroll_attempts = 15

            while scroll_attempts < max_scroll_attempts:
                listings = page.query_selector_all('a[href*="/cars-for-sale/vehicle/"]')
                current_count = len(listings)
                logging.error(f"[AutoTrader] Scroll attempt {scroll_attempts + 1}: Found {current_count} listings")

                if current_count >= max_results or current_count == last_listing_count:
                    if current_count == last_listing_count and current_count > 0:
                        break
                    break

                last_listing_count = current_count
                scroll_height = scroll_attempts * 800 + 1000
                page.evaluate(f'window.scrollBy(0, {scroll_height})')
                time.sleep(1.5)
                scroll_attempts += 1

            page.evaluate('window.scrollTo(0, 0)')
            time.sleep(2)

            # Check for bot detection
            page_text = page.inner_text('body')
            if any(indicator in page_text.lower() for indicator in ['page unavailable', 'incident number:', 'we\'re sorry']):
                logging.error(f"[AutoTrader] Bot detected - returning empty")
                return []

            # Deduplicate and extract listings
            listings = page.query_selector_all('a[href*="/cars-for-sale/vehicle/"]')
            logging.error(f"[AutoTrader] Found {len(listings)} vehicle listing links")

            seen_vehicle_ids = set()
            for listing in listings:
                url = listing.get_attribute('href') or ''
                clean_url = url.split('#')[0]
                vehicle_id_match = re.search(r'/vehicle/(\d+)', clean_url)
                if vehicle_id_match:
                    vehicle_id = vehicle_id_match.group(1)
                    if vehicle_id not in seen_vehicle_ids:
                        seen_vehicle_ids.add(vehicle_id)
                        # Extract listing data...
                        link_text = listing.inner_text()
                        title = link_text.strip()

                        if not title or len(title) < 10:
                            continue

                        # Get price and mileage from ancestor text
                        all_text = ''
                        for ancestor_level in range(1, 13):
                            try:
                                ancestor = listing.evaluate(f'el => {{ let p = el; for(let i=0; i<{ancestor_level}; i++) p = p?.parentElement; return p?.innerText; }}')
                                if ancestor and len(ancestor) > 100:
                                    all_text = ancestor
                                    break
                            except:
                                continue

                        price_match = re.search(r'\$\s*([\d,]+)', all_text)
                        price = extract_number(price_match.group(1)) if price_match else 0

                        if price == 0:
                            for match in re.finditer(r'(\d{1,2},\d{3})', all_text):
                                potential_price = extract_number(match.group(1))
                                if 5000 <= potential_price <= 500000:
                                    price = potential_price
                                    break

                        mileage_match = re.search(r'(\d+)\s*mi\b', all_text, re.IGNORECASE)
                        mileage = int(mileage_match.group(1)) if mileage_match else 0

                        # Get image from ancestor elements
                        image = ''
                        try:
                            img_src = listing.evaluate('''
                                el => {
                                    for (let level = 1; level <= 8; level++) {
                                        let ancestor = el;
                                        for (let i = 0; i < level; i++) {
                                            if (ancestor) ancestor = ancestor.parentElement;
                                        }
                                        if (!ancestor) continue;
                                        const img = ancestor.querySelector('img');
                                        if (img && img.src && img.src.includes('autotrader.com')) {
                                            return img.src;
                                        }
                                    }
                                    return '';
                                }
                            ''')
                            if img_src:
                                image = img_src
                        except:
                            pass

                        if price > 0:
                            results.append({
                                'name': title,
                                'price': price,
                                'mileage': mileage,
                                'image': image,
                                'retailer': 'AutoTrader',
                                'url': f"https://www.autotrader.com{clean_url}" if not clean_url.startswith('http') else clean_url,
                                'location': 'AutoTrader'
                            })
                            logging.error(f"[AutoTrader] {title[:30]} - ${price:,}")

                        if len(results) >= max_results:
                            break

        except Exception as e:
            logging.error(f"[AutoTrader] CDP scraping error: {e}")

        return results


def scrape_with_camoufox(query: str, max_results: int, os_choice: str = None):
    """
    Scrape using Playwright Firefox (formerly Camoufox).
    Name kept for compatibility but now uses regular Firefox for better fingerprint.
    """
    results = []

    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        logging.error("[AutoTrader] Playwright not available!")
        return None

    with sync_playwright() as p:
        browser = p.firefox.launch(
            headless=False,
            firefox_user_prefs={"dom.webdriver.enabled": False}
        )

        context = browser.new_context(viewport={'width': 1920, 'height': 1080})
        page = context.new_page()

        logging.error(f"[AutoTrader] Using Playwright Firefox...")

        if query.startswith('http'):
            search_url = query
        else:
            search_terms = query.replace(' ', '-').lower()
            search_url = f"https://www.autotrader.com/cars-for-sale/{search_terms}?searchRadius=500&zip=94401"

        logging.error(f"[AutoTrader] Navigating to: {search_url}")
        try:
            page.goto(search_url, timeout=60000)
            time.sleep(5)
        except Exception as e:
            logging.error(f"[AutoTrader] Navigation error: {e}")
            time.sleep(5)

        page_text = page.inner_text('body')

        try:
            screenshot_path = f"/tmp/autotrader_debug_{os.getpid()}.png"
            page.screenshot(path=screenshot_path)
            logging.error(f"[AutoTrader] Debug screenshot: {screenshot_path}")
        except:
            pass

        logging.error(f"[AutoTrader] Page text preview: {page_text[:200]}")

        if any(keyword in page_text.lower() for keyword in ['unavailable', 'incident number', 'blocked']):
            logging.error(f"[AutoTrader] BOT DETECTED - retry needed")
            browser.close()
            return None

        logging.error(f"[AutoTrader] Page loaded successfully, extracting...")

        # Check for "no results" page first
        no_results_indicators = ['no results', 'no vehicles found', 'no exact matches', 'try your search again']
        page_text_lower = page_text.lower()
        if any(indicator in page_text_lower for indicator in no_results_indicators):
            logging.error(f"[AutoTrader] No results page detected - returning empty list")
            browser.close()
            return []  # Empty results, NOT an error

        try:
            page.wait_for_selector('[data-cmp="inventoryListing"]', timeout=10000)
        except:
            logging.error(f"[AutoTrader] No listings found or timeout - may be no results or page structure changed")

        # Get only main search result listings, not "Recommended" or "Similar" sections
        # AutoTrader wraps recommendations in sections with headers like "Similar Vehicles"
        listing_cards = page.query_selector_all('[data-cmp="inventoryListing"]')
        logging.error(f"[AutoTrader] Found {len(listing_cards)} total listing cards")

        # Filter out cards in recommendation/similar sections
        main_cards = []
        for card in listing_cards:
            try:
                is_recommendation = card.evaluate('''(el) => {
                    let ancestor = el;
                    for (let i = 0; i < 10; i++) {
                        ancestor = ancestor.parentElement;
                        if (!ancestor) break;
                        const text = ancestor.getAttribute('data-cmp') || '';
                        const heading = ancestor.querySelector('h2, h3');
                        const headingText = heading ? heading.innerText.toLowerCase() : '';
                        if (text.includes('recommend') || text.includes('similar') ||
                            headingText.includes('similar') || headingText.includes('recommend') ||
                            headingText.includes('you might')) {
                            return true;
                        }
                    }
                    return false;
                }''')
                if not is_recommendation:
                    main_cards.append(card)
            except:
                main_cards.append(card)  # Include on error

        if len(main_cards) < len(listing_cards):
            logging.error(f"[AutoTrader] Filtered {len(listing_cards) - len(main_cards)} recommendation cards, {len(main_cards)} main results")

        for idx, card in enumerate(main_cards):
            if idx >= max_results:
                break
            try:
                # Title - h2 with data-cmp=subheading
                title_elem = card.query_selector('h2[data-cmp="subheading"]')
                title = title_elem.inner_text().strip() if title_elem else "Unknown"

                # Price - div with data-cmp=firstPrice (just number, no $ sign)
                price_elem = card.query_selector('div[data-cmp="firstPrice"]')
                price_text = price_elem.inner_text().strip() if price_elem else ""
                price = int(price_text.replace(',', '')) if price_text and price_text.replace(',', '').isdigit() else 0

                # Mileage - in the list items, look for text containing "mi"
                mileage = 0
                list_items = card.query_selector_all('ul[data-cmp="list"] li')
                for item in list_items:
                    text = item.inner_text().strip()
                    if 'mi' in text.lower():
                        # Extract number from "65K mi" or "12,345 mi"
                        num_match = re.search(r'([\d,]+(?:\.\d+)?)\s*[kK]?\s*mi', text, re.IGNORECASE)
                        if num_match:
                            num_str = num_match.group(1).replace(',', '')
                            # Handle K suffix
                            if 'k' in text.lower():
                                mileage = int(float(num_str) * 1000)
                            else:
                                mileage = int(float(num_str))
                        break

                # Location - try various selectors
                location = "Unknown"
                location_selectors = [
                    '[data-cmp="listingSummaryLocationText"]',
                    '.dealer-name',
                    '.location-text',
                    '[data-cmp="dealerName"]'
                ]
                for selector in location_selectors:
                    location_elem = card.query_selector(selector)
                    if location_elem:
                        location = location_elem.inner_text().strip()
                        break

                # Image - img with data-cmp="inventoryImage"
                image = ""
                img_elem = card.query_selector('img[data-cmp="inventoryImage"]')
                if img_elem:
                    image = img_elem.get_attribute('src') or ""
                    # Clean up HTML entities in URL
                    image = image.replace('&amp;', '&')

                # Link - AutoTrader listings always have a link
                # HTML structure: <a href="..."><h2 data-cmp="subheading">TITLE</h2></a>
                # The anchor wraps the h2, so we need to find h2 first, then get parent anchor
                link = ""
                try:
                    # Find the h2 element with data-cmp="subheading"
                    h2_elem = card.query_selector('h2[data-cmp="subheading"]')

                    if h2_elem:
                        # Get the parent anchor tag of the h2 element
                        # The anchor wraps the h2, so we use evaluate to call closest() on the h2
                        link_result = h2_elem.evaluate('(el) => { const anchor = el.closest("a"); return anchor ? anchor.href : null; }')
                        if link_result:
                            link = link_result
                            logging.error(f"[AutoTrader] Listing {idx}: Found URL via h2 parent anchor: {link[:80]}...")
                        else:
                            logging.error(f"[AutoTrader] Listing {idx}: h2 found but no parent anchor")
                    else:
                        logging.error(f"[AutoTrader] Listing {idx}: No h2[data-cmp='subheading'] found in card")

                    # Fallback: try finding anchor with vehicle href directly
                    if not link:
                        fallback_links = card.query_selector_all('a[href*="/cars-for-sale/vehicle/"]')
                        if fallback_links:
                            link = fallback_links[0].get_attribute('href') or ""
                            logging.error(f"[AutoTrader] Listing {idx}: Found URL via fallback (vehicle href)")

                except Exception as e:
                    logging.error(f"[AutoTrader] Error extracting link for listing {idx}: {e}")

                # If still no link, log details for debugging
                if not link:
                    logging.error(f"[AutoTrader] Listing {idx}: NO URL FOUND - this should not happen")
                    # Still add the listing - user wants all listings
                    link = f"https://www.autotrader.com/error-no-url-{idx}"

                vehicle = {
                    "name": title,           # Changed from "title" to "name" for backend compatibility
                    "price": price,
                    "mileage": mileage,
                    "location": location,
                    "url": link,             # Changed from "link" to "url" for backend compatibility
                    "image": image,          # Extracted from img[data-cmp="inventoryImage"]
                    "retailer": "AutoTrader",  # Explicit retailer name
                    "source": "autotrader"
                }

                results.append(vehicle)
                logging.error(f"[AutoTrader] Extracted: {title} - ${price:,}")

            except Exception as e:
                logging.error(f"[AutoTrader] Error extracting vehicle {idx}: {e}")
                continue

        browser.close()

    return results

def scrape_autotrader(query: str, max_results: int = 10, cdp_url: str = "http://localhost:9222", enable_vpn: bool = False):
    """
    Main scrape function.

    VPN is now handled by the worker process. This scraper returns:
    - List of results on success
    - None on bot detection (worker will retry with VPN)

    Args:
        query: Search URL or structured query
        max_results: Maximum number of results to return
        cdp_url: CDP URL (not used, kept for compatibility)
        enable_vpn: Ignored (worker handles VPN)

    Returns:
        List of vehicle listings, or None if bot detected
    """
    # If Camoufox is not available, fall back to CDP
    if not CAMOUFOX_AVAILABLE:
        logging.error(f"[AutoTrader] Camoufox not available, falling back to CDP...")
        return scrape_with_playwright_cdp(query, max_results, cdp_url)

    # Try scraping with Camoufox
    # Bot detection returns None for worker to retry with VPN
    return scrape_with_camoufox(query, max_results, os_choice='windows')


def main():
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: scrape-autotrader.py <URL or search query or structured> [max_results]",
            "examples": [
                "scrape-autotrader.py 'GMC Sierra Denali black'",
                "scrape-autotrader.py 'https://www.autotrader.com/cars-for-sale/...'",
                "scrape-autotrader.py '{\"structured\": {...}}'  # From parse_vehicle_query.py"
            ],
            "note": "VPN retry is handled by the worker process. This scraper returns None on bot detection."
        }))
        sys.exit(1)

    query_input = sys.argv[1]
    max_results = int(sys.argv[2]) if len(sys.argv) > 2 else 10

    # Parse input to determine format
    query = query_input

    if query_input.strip().startswith('{'):
        # Structured JSON format from parse_vehicle_query.py
        try:
            structured_data = json.loads(query_input)
            if 'structured' in structured_data:
                # Use adapter to convert structured to AutoTrader query
                query = adapt_structured_to_autotrader(structured_data['structured'])
            else:
                query = query_input  # Use as-is
        except json.JSONDecodeError:
            query = query_input
    elif not query_input.startswith('http'):
        # Plain text query - use as-is
        query = query_input

    try:
        result = scrape_autotrader(query, max_results)

        # Handle different return types
        # - None: bot detection (worker will retry with VPN)
        # - []: no results found (valid empty response)
        # - list with items: success
        if result is None:
            # Bot detection - return error dict for worker to retry
            print(json.dumps({"error": "Bot detection - retry with VPN"}))
            sys.stdout.flush()
            sys.exit(1)
        elif not result or (isinstance(result, list) and len(result) == 0):
            # No results - return empty list (NOT an error)
            print("[]")
            sys.stdout.flush()
        else:
            # Success - return listings
            output = json.dumps(result, indent=2)
            print(output)
            sys.stdout.flush()
            # Also write to temp file as backup
            with open(f"/tmp/scraper_output_{os.getpid()}.json", "w") as f:
                f.write(output)
                f.flush()
    except Exception as e:
        print(json.dumps({"error": str(e)}))
        sys.stdout.flush()
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
